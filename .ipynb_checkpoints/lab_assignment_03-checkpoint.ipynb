{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The third Lab-assignment (07/22/2022 11:59'AM' - 07/26/2022 11:59PM, 50 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlease write you answer here:\\n\\nDomain problem: \\nWhat is the change in median housing price at different counties in Texas? Which county has highest rate of ROI?\\n\\nWhat kind of data should be collected? \\nWe should collect historical median price of house of all the counties in Texas\\n\\nHow many data needed for the analysis\\nWe monthly median housing price of each county from 10 to 30 years range.\\n\\nThe detail steps for collecting and save the data\\nI will be collecting data from https://www.recenter.tamu.edu/data/housing-activity/#!/activity/County/Willacy_County\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
    "\n",
    "'''\n",
    "Please write you answer here:\n",
    "\n",
    "Domain problem: \n",
    "What is the change in median housing price at different Texas Metropolitan Statistical Areas (MSA) ? \n",
    "Which MSA has highest rate of ROI?\n",
    "\n",
    "What kind of data should be collected? \n",
    "We should collect historical median price of house of all MSA in Texas\n",
    "\n",
    "How many data needed for the analysis\n",
    "We need monthly median housing price of each MSA from 10 to 30 years range.\n",
    "\n",
    "The detail steps for collecting and save the data\n",
    "I will be collecting data from https://www.recenter.tamu.edu/data/housing-activity#!/activity/MSA/ which\n",
    "have historical data of median house prices in Texas. \n",
    "\n",
    "I am using Selenium, Selenium.webdriver.chrome.service, \n",
    "Webdriver_manager.chrome, bs4 and pandas library to collect data\n",
    "\n",
    "There are two main steps:\n",
    "\n",
    "1) Collect tabular data from each drop-down option for MSA. Which would mean all tablular data of Texas MSA. \n",
    "- First, I have read base website and read all the select option values to get all MSA and store it in a list\n",
    "- I created a function to collect tabular data for the given url and return dataframe with table information,\n",
    "  I had to use selenium becuase I was not getting exact value using bs4\n",
    "- Then, i looped through all MSA list on get data function and appended everything on one dataframe\n",
    "\n",
    "\n",
    "2) Combine data from different Texas MSA into one for analysis\n",
    "- I looped through all MSA list on get data function and appended everything on one dataframe\n",
    "- Export it to a csv file for analysis\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 (30 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Getting data from Abilene\n",
      "\n",
      "Getting data from Amarillo\n",
      "\n",
      "Getting data from Austin-Round_Rock\n",
      "\n",
      "Getting data from Beaumont-Port_Arthur\n",
      "\n",
      "Getting data from Brownsville-Harlingen\n",
      "\n",
      "Getting data from College_Station-Bryan\n",
      "\n",
      "Getting data from Corpus_Christi\n",
      "\n",
      "Getting data from Dallas-Fort_Worth-Arlington\n",
      "\n",
      "Getting data from Dallas-Plano-Irving\n",
      "\n",
      "Getting data from El_Paso\n",
      "\n",
      "Getting data from Fort_Worth-Arlington\n",
      "\n",
      "Getting data from Houston-The_Woodlands-Sugar_Land\n",
      "\n",
      "Getting data from Killeen-Temple\n",
      "\n",
      "Getting data from Laredo\n",
      "\n",
      "Getting data from Longview\n",
      "\n",
      "Getting data from Lubbock\n",
      "\n",
      "Getting data from McAllen-Edinburg-Mission\n",
      "\n",
      "Getting data from Midland\n",
      "\n",
      "Getting data from Odessa\n",
      "\n",
      "Getting data from San_Angelo\n",
      "\n",
      "Getting data from San_Antonio-New_Braunfels\n",
      "\n",
      "Getting data from Sherman-Denison\n",
      "\n",
      "Getting data from Texarkana\n",
      "\n",
      "Getting data from Tyler\n",
      "\n",
      "Getting data from Victoria\n",
      "\n",
      "Getting data from Waco\n",
      "\n",
      "Getting data from Wichita_Falls\n",
      "         Date Sales DollarVolume AveragePrice MedianPrice TotalListings  \\\n",
      "0    Jan 1990   103    4,791,766       46,522      56,214           765   \n",
      "1    Feb 1990    61    2,945,873       48,293      66,072           981   \n",
      "2    Mar 1990    85    4,218,975       49,635      62,551         1,042   \n",
      "3    Apr 1990    95    4,135,730       43,534      57,094         1,044   \n",
      "4    May 1990   106    4,803,602       45,317      56,038           971   \n",
      "..        ...   ...          ...          ...         ...           ...   \n",
      "385  Feb 2022   158   28,015,084      177,311     167,000           191   \n",
      "386  Mar 2022   196   39,292,228      200,471     176,950           208   \n",
      "387  Apr 2022   193   42,216,585      218,739     190,000           194   \n",
      "388  May 2022   220   47,499,679      215,908     181,250           219   \n",
      "389  Jun 2022   205   45,154,745      220,267     194,250           310   \n",
      "\n",
      "    MonthsInventory            MSA  \n",
      "0               7.4        Abilene  \n",
      "1              12.0        Abilene  \n",
      "2              12.6        Abilene  \n",
      "3              12.2        Abilene  \n",
      "4              10.8        Abilene  \n",
      "..              ...            ...  \n",
      "385             1.0  Wichita_Falls  \n",
      "386             1.1  Wichita_Falls  \n",
      "387             1.0  Wichita_Falls  \n",
      "388             1.1  Wichita_Falls  \n",
      "389             1.6  Wichita_Falls  \n",
      "\n",
      "[10529 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "\n",
    "\n",
    "# Import library\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_data(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # get all the table data\n",
    "    table1 = soup.find('table', class_='ng-scope')\n",
    "\n",
    "    current_MSA = url.split('/MSA/', 2)[1]\n",
    "    print(\"Getting data from \"+ current_MSA )\n",
    "\n",
    "\n",
    "    # Obtain every title of the column\n",
    "    headers = []\n",
    "    for i in table1.find_all('th'):\n",
    "        title = i.text\n",
    "        headers.append(title)\n",
    "\n",
    "    mydata = pd.DataFrame(columns=headers)\n",
    "\n",
    "    # Create a loop to fill mydata\n",
    "    for j in table1.find_all('tr')[1:]:\n",
    "        row_data = j.find_all('td')\n",
    "        row = [i.text for i in row_data]\n",
    "        length = len(mydata)\n",
    "        mydata.loc[length] = row\n",
    "        \n",
    "    mydata[\"MSA\"] = current_MSA\n",
    "    return mydata\n",
    "\n",
    "\n",
    "def get_all_MSA(url):\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    # get all the table data\n",
    "    table1 = soup.find('option', class_='ng-options')\n",
    "\n",
    "    # Find select tag\n",
    "    select_tag = soup.find_all(\"select\")\n",
    "    #print(select_tag[1])\n",
    "\n",
    "    options = select_tag[1].find_all(\"option\")\n",
    "    MSA_list = []\n",
    "    # Iterate through all option tags and get inside text\n",
    "    for option in options:\n",
    "        MSA_list.append(option.text)\n",
    "\n",
    "    url2 = \"https://www.recenter.tamu.edu/data/housing-activity#!/activity/MSA/\"\n",
    "    \n",
    "    \n",
    "    MSA_list_new= []\n",
    "    url_list = []\n",
    "    for names in MSA_list:\n",
    "        text = names\n",
    "        MSA_list_new.append(url2 + text.replace(' ', '_'))\n",
    "        \n",
    "    return MSA_list_new\n",
    "\n",
    "\n",
    "#Get the base url\n",
    "url = \"https://www.recenter.tamu.edu/data/housing-activity#!/activity/MSA/Abilene\"\n",
    "\n",
    "#Get List of all MSA in Texas\n",
    "MSA = get_all_MSA(url)\n",
    "\n",
    "#Initialize dataframe\n",
    "final_data = pd.DataFrame()\n",
    "\n",
    "#loop through MSA list to get data and store it inside one single dataframe\n",
    "for item in MSA:\n",
    "    final_data =  pd.concat([final_data, get_data(item)])\n",
    "\n",
    "# export to csv\n",
    "final_data.to_csv('Texas_MSA_Housing.csv', index=False)\n",
    "\n",
    "print(final_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nPlease write you answer here:\\n\\nThe second hand dataset I came across Kaggle is:\\nhttps://www.kaggle.com/datasets/georgesaavedra/covid19-dataset\\n\\nThe data quality is fair. Few qualities I found with this dataset are:\\nNull/missing values for many countries\\nDuplicate values\\nFew countries does not have a row for certain dates\\n\\nMy strategy to clean such dataset would be to:\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
    "'''\n",
    "\n",
    "Please write you answer here:\n",
    "\n",
    "The second hand dataset I came across Kaggle is:\n",
    "https://www.kaggle.com/datasets/georgesaavedra/covid19-dataset\n",
    "\n",
    "The data quality is fair. Few qualities I found with this dataset are:\n",
    "- Null/missing values for many countries\n",
    "- Duplicate values\n",
    "- Few countries does not have a row for certain dates\n",
    "- I did not find any mistyped information and column datatypes are aligned\n",
    "\n",
    "My strategy to clean such dataset would be to:\n",
    "- Check for duplicates and remove them\n",
    "- Check for null values and fill missing values with mean of the same group when possible\n",
    "- Remove unnecessary column that I will not be using for analysis\n",
    "- Add additional calculated column based on existing column\n",
    "- Depending on the use case, include or exclude outliers if any\n",
    "- use aggregated data for analysis\n",
    "- reiterate through these steps as needed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c6a7f839effdddf35473dacbb9a8184e57ebbba5133cab03b12e4f28d4f0d0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
